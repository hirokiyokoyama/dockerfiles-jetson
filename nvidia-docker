#!/bin/bash
#Jason T. 2-6-2018

# Check specifically for the run command
if [[ $# -ge 2 && $1 == "run" ]]; then
    # Tell docker to share the following folders with the base system
    # This allows the docker containers to find CUDA, cuDNN, TensorRT
    # ldd /usr/lib/python3.6/dist-packages/cv2.cpython-36m-aarch64-linux-gnu.so | cut -d' ' -f 3 | grep /usr/lib/aarch64-linux-gnu
    LIB_MAPS="/usr/local/cuda \
              /usr/local/cuda/lib64 \
	      /usr/lib/aarch64-linux-gnu \
	      /usr/lib/python3.6/dist-packages/cv2.cpython-36m-aarch64-linux-gnu.so \
	      /lib/aarch64-linux-gnu" 
		 
    # GPU device sharing is for a docker container to have access to
    # system devices as found in the /dev directory
    DEVICES="/dev/nvhost-ctrl \
             /dev/nvhost-ctrl-gpu \
             /dev/nvhost-prof-gpu \
             /dev/nvmap \
             /dev/nvhost-gpu \
             /dev/nvhost-as-gpu"
	
	# There is a need to map the libraries inside the container
	# in the exact way programs would expect to find them as
	# though the TX2 were operating without containers
	#LIBS="-v /lib/aarch64-linux-gnu/libm-2.27.so:/usr/lib/aarch64-linux-gnu/libm.so.6"
	LIBS=""
	for lib in $LIB_MAPS; do
		LIBS="$LIBS -v $lib:$lib"
	done
	for lib in `ls /usr/lib/libopencv_*`; do
		LIBS="$LIBS -v $lib:$lib"
	done
	
	DEVS=""
	for dev in $DEVICES; do
		DEVS="$DEVS --device=$dev"
	done

	ENVS="-e LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/lib/aarch64-linux-gnu:/usr/lib/aarch64-linux-gnu/tegra"
	ENVS="$ENVS -e PYTHONPATH=/usr/lib/python3.6/dist-packages"
	
	#echo "docker run $LIBS $DEVS ${@:2}"
	docker run $LIBS $DEVS $ENVS ${@:2}
else
    # Run command not found, run everything straight through docker
    #echo "docker $@"
	docker $@
fi

